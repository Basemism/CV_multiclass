{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3680 images and 3680 masks\n"
     ]
    }
   ],
   "source": [
    "dim = 128  \n",
    "num_classes = 3  # Trimap {1,2,3}\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "train_images_dir = f'./trainval_{dim}/images'\n",
    "train_masks_dir  = f'./trainval_{dim}/annotations'\n",
    "\n",
    "image_paths = sorted(glob.glob(os.path.join(train_images_dir, '*.png')))\n",
    "mask_paths  = sorted(glob.glob(os.path.join(train_masks_dir, '*.png')))\n",
    "\n",
    "print(f'Found {len(image_paths)} images and {len(mask_paths)} masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, img_size):\n",
    "        \"\"\"\n",
    "        image_paths: list of file paths to input images.\n",
    "        mask_paths: list of file paths to corresponding trimap masks.\n",
    "        img_size: desired image size (both width and height).\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image with OpenCV (BGR -> convert to RGB)\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found at {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        image = image.astype(np.float32) / 255.0  \n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        \n",
    "        # Load mask in grayscale and resize using nearest-neighbor to preserve labels.\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Mask not found at {mask_path}\")\n",
    "        mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "        # Shift labels: from {1,2,3} to {0,1,2}\n",
    "        mask = mask - 1\n",
    "        \n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        mask_tensor = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image_tensor, mask_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 3312  |  Validation images: 368\n"
     ]
    }
   ],
   "source": [
    "train_img_paths, val_img_paths, train_mask_paths, val_mask_paths = train_test_split(\n",
    "    image_paths, mask_paths, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "print(f\"Training images: {len(train_img_paths)}  |  Validation images: {len(val_img_paths)}\")\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = myDataset(train_img_paths, train_mask_paths, dim)\n",
    "val_dataset   = myDataset(val_img_paths, val_mask_paths, dim)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Convolution => BatchNorm => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = DoubleConv(3, 16)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(16, 32))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(32, 64))\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv1 = DoubleConv(256, 128)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(128, 64)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(64, 32)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(32, 16)\n",
    "        \n",
    "        self.outc = nn.Conv2d(16, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)       # (B, 16, H, W)\n",
    "        x2 = self.down1(x1)    # (B, 32, H/2, W/2)\n",
    "        x3 = self.down2(x2)    # (B, 64, H/4, W/4)\n",
    "        x4 = self.down3(x3)    # (B, 128, H/8, W/8)\n",
    "        x5 = self.down4(x4)    # (B, 256, H/16, W/16)\n",
    "        \n",
    "        u1 = self.up1(x5)      # (B, 128, H/8, W/8)\n",
    "        # Concatenate skip connection from x4.\n",
    "        u1 = torch.cat([u1, x4], dim=1)\n",
    "        u1 = self.conv1(u1)\n",
    "        \n",
    "        u2 = self.up2(u1)      # (B, 64, H/4, W/4)\n",
    "        u2 = torch.cat([u2, x3], dim=1)\n",
    "        u2 = self.conv2(u2)\n",
    "        \n",
    "        u3 = self.up3(u2)      # (B, 32, H/2, W/2)\n",
    "        u3 = torch.cat([u3, x2], dim=1)\n",
    "        u3 = self.conv3(u3)\n",
    "        \n",
    "        u4 = self.up4(u3)      # (B, 16, H, W)\n",
    "        u4 = torch.cat([u4, x1], dim=1)\n",
    "        u4 = self.conv4(u4)\n",
    "        \n",
    "        logits = self.outc(u4)  # (B, num_classes, H, W)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(num_classes=num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()  # expects logits [B, C, H, W] and target [B, H, W]\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # (B, num_classes, H, W)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "    return epoch_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total_pixels = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            epoch_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == masks).sum().item()\n",
    "            total_pixels += torch.numel(masks)\n",
    "    accuracy = correct / total_pixels\n",
    "    return epoch_loss / len(loader.dataset), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]  Train Loss: {train_loss:.4f}  \"\n",
    "          f\"Val Loss: {val_loss:.4f}  Val Pixel Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save the best model.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"unet_model_{dim}_epochs_{num_epochs}.pth\")\n",
    "\n",
    "# Save the trained model.\n",
    "torch.save(model.state_dict(), f\"unet_model_{dim}_epochs_{num_epochs}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test images and save the results.\n",
    "import pickle\n",
    "\n",
    "#create output folder\n",
    "\n",
    "if not os.path.exists('output_unet_test'):\n",
    "    os.makedirs('output_unet_test')\n",
    "\n",
    "# Load the best model.\n",
    "model.load_state_dict(torch.load(\"unet_model_128_epochs_50.pth\"))\n",
    "\n",
    "\n",
    "with open('test_images_paths.pkl', 'rb') as f:\n",
    "    test_images = pickle.load(f)\n",
    "\n",
    "with open('test_trimap_paths.pkl', 'rb') as f:\n",
    "    test_trimap = pickle.load(f)\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "\n",
    "    image = cv2.imread(test_images[i])\n",
    "    trimap = cv2.imread(test_trimap[i], interpolation=cv2.INTER_NEAREST)\n",
    "    trimap = trimap - 1\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (dim, dim))\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "    image_tensor = torch.from_numpy(image).float().unsqueeze(0)\n",
    "\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        output = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    output = output + 1\n",
    "    output = output.astype(np.uint8)\n",
    "\n",
    "    output = cv2.resize(output, (trimap.shape[1], trimap.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    cv2.imwrite(f'output_unet_test/output_{i}.png', output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
